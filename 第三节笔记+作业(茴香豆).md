# 1. 视频笔记
## 1. 关于RAG
对于询问大模型中没有的内容时，由于没有学习过，大模型会不知道或者胡乱编造答案。对此，传统的做法是去采集新增语料，通过微调的方式，对模型进行续训。
这种传统做法现实中会出现问题
  1. 知识更新太快
  2. 语料库太大，训练成本高
  3. 难以收集等

针对这种现象，RAG技术可以很好解决————没有额外训练的情况下，能回答不在知识库的问题了。 怎么实现的呢？——————了解RAG

**RAG**  Retrieval augmented generation
1. 定义：检索增项生成模型

2. 通过结果检索和生成的过程 ， 增强了语言模型在解锁特定问题的性能。这个过程涉及从外部知识源检索相关信息，并将这些信息用于指导语言模型生成更准确更丰富的回答。

3. 更通俗的说：RAG就是一个**搜索引擎**————用户输入的内容作为索引————外部知识库中搜寻相关内容————结合LLM的能力生成回答

4. 最大优势：解决LLM在处理知识密集型任务时可能遇到的挑战。提供更准确的回答、降低成本、实现外部记忆————————没有训练过程，就是检索，成本很低

5. RAG应用：问答系统、文本生成、信息检索、图片描述

6. **RAG工作原理**：
   1. 索引 indexing ：处理外部知识——把知识源 （文档、网页）分割成chunk，编码成向量，存储在Vector-DB中。
   2. 检索 Retrieval: 负责接收用户问题，接收到后也编码成向量，在vectorDB中找到与之最相关的文档块。
   3. 生成 Generation: 将检索到的文档块与原始问题一起作为提示，输入到LLM中，生成回答。
   4. ![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/b2e5fc67-102f-44d7-815c-2480517380e2)
   
7. **向量数据库  Vector DB**
   1. 是RAG中，专门储存外部数据的地方。
   2. 数据存储：将文本及其他数据通过预训练的模型转换为固定长度的向量表示，这些向量能够很好捕捉文本的语义信息。
   3. 相似性检索：根据用户的查询向量，使向量数据库快速找出最相关的向量的过程。
      1. 用余弦相似度或者其他相似度方式来完成。
      2. 根据score进行排序，最相关的文档将被用于后续文本生成。
   4. 向量表示的优化：使用更高级的文本编码技术，句子嵌入或段落嵌入；对数据库进行优化以支持大规模向量搜索；包括尝试不同的嵌入方法；
      1. 直接影响RAG的好坏
   
8. **RAG的发展历程**
   1. native RAG : 即上述RAG 流程
   2. advanced RAG ：在native RAG的基础上，在检索和生成模块有诸多改进：（摘要生成、内容推荐等应用）
      1. pre-retrieval optimization：比如句子窗口检索
      2. post-retrieval optimization：比如对结果进行重排
      3. hybrid search combining different retrieval methods：混合搜索结合不同检索方法
   4. modular RAG：强调灵活性和模块化。允许集成不同组件和技术以适应特定任务或需求。它更加明确地区分检索和生成阶段，使得可以针对每个部分使用专门的模型和技术。（多模态任务，对话系统等应用）
   
9. **RAG常见优化方法**
    1. 提升数据库质量
         1. 嵌入式优化  embedding optimization
            1. 考虑通过结果稀疏编码器、密集检索器以及多任务的方式增强嵌入的性能
         2. 索引优化  indexing optimization
            1. 增加数据粒度、优化索引结构、添加metadata、对齐优化和混合检索等策略来提升索引质量
    2. 前检索、后检索（检索之前和之后）-advanced RAG
       1. 查询扩展和转换：时的用户原始问题更清晰、更适合检索任务
          1. 采用多查询方法，通过LLM的prompt engineer来扩展查询
       2. 上下文管理：通过重排和上下文选择压缩来减少检索的冗余信息，并提高大模型的处理效率
          1. 例子1：使用小一点的语言模型来检测和一出不重要的标记，
          2. 例子2：使用训练信息提取器和信息压缩器
    3. 改善检索技术本身
       1. 迭代检索：根据检索结果多次迭代检索知识，为大模型生成提供全面的知识基础
       2. 递归检索：通过迭代细化查询来改进搜索结果的深度和相关性，使用了链式推理指导检索过程，并根据检索结果细化推理过程。
       3. 自适应检索：让大模型能够自主地在最佳时机去决定它所要检索的内容
          1. flair
          2. self RAG
    4. LLM fine-turning
       1. 根据场景和数据特征对大模型进行定向微调
       2. 根据大模型对于检索或生成的参与进行有针对性的微调
    
10. **RAG 和 微调的比较**

    1. RAG和微调都是常见的LLM性能优化方式。当然还有提示工程

    2. 微调 fine-tuning

        1. 通过微调能发挥预训练语言模型在特定场景中的专业能力
        2. 核心：在新的任务和数据集上对预训练模型的参数进行微小的调整，使其能精准契合目标场景的需求。
        3. 它不是从头重新训练一个模型
        4. 例子： 
            1. 目的：对电影评论进行正负面情绪判断。
            2. 过程：选择像 GPT-3 这样在海量文本上预训练过的 PLM————使用标注好的电影评论数据集对其进行微调。
            3. 结果：通过这一调整权重的过程，原本通用的语义模型就被"赋能"了情感分析的专业技能，可以高精度地捕捉评论中的情感倾向。

    3.  前两者比较综述：

        1. Fine-Tuning 微调：赋能语言模型任务专注的秘钥

           微调技术的核心功能，是通过对预训练语言模型权重的精细调校，将其通用的语义认知能力定制化为某一特定任务或领域的专注技能。这种"通过少量数据开启定制化"的高效方式，让语言模型得以脱胎于通用知识的卵壳，孵化出应对特殊场景的专业能力。

        2. RAG：外延语言模型认知边界的知识加持

           而检索增强生成则是另一种智能范式，赋予了语言模型高效获取和利用外部异构知识的能力。通过智能检索系统，语言模型可在生成响应前，从海量专业知识库中获取与之相关的补充信息，从而突破了其受限于训练数据集的知识边界，使输出更加准确、专业、内容丰富。

    4. https://www.53ai.com/news/qianyanjishu/2097.html

11. 对LLM常用优化方式的比较：prompt-engineering vs fine tuning vs RAG

     1. https://myscale.com/blog/zh/prompt-engineering-vs-finetuning-vs-rag/
     2. 比较原则：
         1. 任务对外部知识的需求
         2. 任务对模型适配度的需求
     3. 提示工程：在面对外部知识需求和模型适配需求上的表现<u>都是比较差</u>的，它不能适应新的知识，同时对于特定任务也难以有很专业的表现。
     4. 微调：是一种对于外部数据需求要求不高，但是对于模型任务适配要求很高的场景的优化手段。
     5. RAG：正好与微调相反，它适用于高外部知识需求，但是对于模型适配度要求不高的场景。
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/69d400f8-0b51-401c-afc0-0dd0abebd029)

12. 评价RAG的效果好坏的指标

     1. 传统：准确率、召回率、 f1 score、 BLEU score 、ROUGE score
     2. RAG评测框架：
         1. 基准测试： RGB 、RECALL、CRUD
         2. 评测工具：RAGAS、ARES、TruLens
## 2. 关于茴香豆

1. 简介
   1. 开源、免费商用
   2. 可完全本地部署
   3. 可用远端大模型
   4. 针对我们常见的通讯软件中的群聊场景进行了很好的工作流的优化
   5. 为群里边用户提供及时准确的技术支持和自动化的问答服务
   6. 茴香豆最主要的适用场景就是<u>智能客服</u>（即时通讯工具中的技术与用户群）
      1. 微信、飞书群用户增加————群内信息繁杂————茴香豆助手能分辨出用户真实意图——定位真正的问题去解答
2. 茴香豆个人知识助手组成
   1. 知识库
      1. 企业内部或者是个人所在领域的专业技术文档
      2. 价目表、产品规格等等这类会实时更新的数据
      3. 支持了Markdown，PDF，word，TXT， Powerpoint 等常用的一些文件的格式
   2. 前端
      1. 一般是微信群或者是飞书群
   3. 后端LLM
      1. 支持本地调用大模型，或者是调用远端大模型的API
         1. 本地：书生谱语和通义千问
         2. 远端： Kimi 、chatGPT 、ChatGLM 的API
3. 茴香豆工作流
   1. 预处理
   2. 拒答工作流
   3. 应答工作流
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/3e3cfa6d-8de4-4c78-9c0a-b5adf0c8bd29)

# 2. 实际操作

### 1. 茴香豆环境

```
1. 创建conda 茴香豆环境
    conda create -n huixiangdou python=3.10
    conda activate huixiangdou
    conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia
2. 软链接
    # 复制BCE模型
    ln -s /root/share/new_models/maidalun1020/bce-embedding-base_v1 /root/models/bce-embedding-base_v1
    ln -s /root/share/new_models/maidalun1020/bce-reranker-base_v1 /root/models/bce-reranker-base_v1
    # 复制大模型参数
    ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-7b /root/models/internlm2-chat-7b
3. 安装茴香豆运行所需依赖
pip install protobuf==4.25.3 accelerate==0.28.0 aiohttp==3.9.3 auto-gptq==0.7.1 bcembedding==0.1.3 beautifulsoup4==4.8.2 einops==0.7.0 faiss-gpu==1.7.2 langchain==0.1.14 loguru==0.7.2 lxml_html_clean==0.1.0 openai==1.16.1 openpyxl==3.1.2 pandas==2.2.1 pydantic==2.6.4 pymupdf==1.24.1 python-docx==1.1.0 pytoml==0.1.21 readability-lxml==0.8.1 redis==5.0.3 requests==2.31.0 scikit-learn==1.4.1.post1 sentence_transformers==2.2.2 textract==1.6.5 tiktoken==0.6.0 transformers==4.39.3 transformers_stream_generator==0.0.5 unstructured==0.11.2
4. 下载茴香豆
    cd /root
    git clone https://github.com/internlm/huixiangdou && cd huixiangdou
    git checkout b9bc427
5. 修改config.ini里的一些系统路径
	在config.ini文件直接vim修改，也可用sed -i 命令修改
    主要修改有三：
    ①用于向量数据库和词嵌入的模型path：/root/models/bce-embedding-base_v1
    ②用于检索的重排序模型path：/root/models/bce-reranker-base_v1
    ③本次选用的大模型path：/root/models/internlm2-chat-7b
6. 创建知识库，在/huixiangdo/repodir中创建，就是下载
git clone https://github.com/internlm/huixiangdou --depth=1 repodir/huixiangdou
7. 增加茴香豆相关的问题到接受问题示例中
cd /root/huixiangdou
mv resource/good_questions.json resource/good_questions_bk.json # 备份一下，然后更改示例
echo '[xxxxxxxxxxxx]'  > /root/huixiangdou/resource/good_questions.json
8. 创建一个测试用的问询列表（.test_query.json），用来测试拒答流程是否起效
echo '[
"huixiangdou 是什么？",
"你好，介绍下自己"
]' > ./test_queries.json
9. 创建RAG检索过程中的向量数据库
# 创建向量数据库存储目录
cd /root/huixiangdou && mkdir workdir 
# 分别向量化知识语料、接受问题和拒绝问题中后保存到 workdir
python3 -m huixiangdou.service.feature_store --sample ./test_queries.json
10. 运行期间报错了，根据错误提示，单独安装一下这个依赖
pip install -U duckduckgo_search
11. 重新运行 会运行成功，出现特别多的log信息。如下图5 和 6
```
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/53477198-301a-47db-92e9-14c4bd08aeb9)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/2d125ab6-63b3-444e-8279-d765318d1fe9)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/b939d98f-64ae-4cca-b6e4-e13a4df9156e)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/ef138bea-66d3-4e9c-a59e-9af79174c0b9)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/7c0bf2c6-d08b-46cc-b659-e62855d59bb7)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/d12b0c21-7059-4a1b-a9a4-40b9474b5944)

如上说明，茴香豆运行成功。成功的搭建起来vector-DB。


### 2. 运行知识助手

```
1. 填入问题
sed -i '74s/.*/    queries = ["huixiangdou 是什么？", "茴香豆怎么部署到微信群", "今天天气怎么样？"]/' /root/huixiangdou/huixiangdou/main.py

2. 运行茴香豆
cd /root/huixiangdou/
python3 -m huixiangdou.main --standalone
```
从下图可以看出，针对不同的问题，即query，每个问题都会进行语法和相关性分析，并打分，但发现打分的评分标准还是比较简单。
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/6d75d730-4896-474e-8c28-1156e6f61587)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/ec0865bf-132c-4488-a652-8ab4250a8cf2)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/bed98625-1d8e-4ddf-b64d-0ec3db3be1fb)


