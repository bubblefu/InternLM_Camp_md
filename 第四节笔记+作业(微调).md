# 1. 笔记

### 1. 为什么要微调

1. foundation 大语言模型 在能够实现“普遍的”“一般性的”任务的“通用”，需要进行预训练。
2. 若想利用通用基座模型到自己下游业务场景中，若没有经过专属微调，直接拿来用的话，其实表现不如领域内已专门训练好的模型。 

### 2. 两种fine tune 范式
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/f0505f42-095c-4be5-8493-64016bbdac3a)

1. **增量预训练**
   1. 基座模型（直接从hugging face下载的模型），让他学习一些<u>**新的知识**</u>
      1. 例子：我的垂直领域的一些常识
      2. 文章、书籍、代码等
   2. 不需要一问一答的学习，而是有监督、有标注的
   3. 例子：世界上第一高峰是珠穆朗玛
2. **指令跟随微调**
   1. 场景：让模型学会对话模板，根据人类的指令进行对话，让模型输出更符合人类喜好偏好的回答
   2. 训练数据：高质量的对话和问答数据
   3. 需要很高质量的标注（问答对）
   4. 例子：问：世界上第一高峰是什么？ 答：是珠穆朗玛峰啊

![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/ae79ada9-b337-455a-9052-0a690365264a)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/66ead99f-a822-4c53-84cb-1298e6ada30d)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/6087316a-8224-4580-afe9-bbeafa2421c0)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/7b329e4e-fb33-4aa5-a5a1-903cb559a03e)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/fdbd8e72-0ec4-4dd0-a8b7-d6eae8c1c8a5)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/5ca2357c-05a9-4c88-8aae-76b66f21ddfd)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/d1ed7515-0a4e-4c6a-8fb1-4673b2a71fe4)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/188950a8-ce30-4b19-9b20-b0d334359319)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/80cd58cc-3cdc-41db-b590-96f804411972)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/56c95d16-b4fc-4ab9-86f0-42b8d32ad080)
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/615ea2c5-e046-472d-bd66-fb738bbd8030)

![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/83456a35-36fa-4df9-961e-8169570fe2d5)


# 2. 实战

# 3. 微调的一些小技巧


Tip 1:Start withaSmall Model(从小模型开始，测试是否一切正常)

Tip 2: Use LoRA or OLoRA(使用 LoRA 或 OLoRA)

Tip 3: Create 10 manual questions(手动创建 10 个问题，测试是否一切正常)

Tip 4: Create datasets manually(手动创建数据集，会有更深的理解)

Tip 5: Start training withjust 100 rows(从只有 100 行开始训练，测试模型是否正常)

Tip 6: Always create a validation data split(始终创建验证数据集)

Tip 7: Start by only training on one GPU(开始时只使用一个GPU进行训练)

Tip 8: Use weights and biases for logging(记录日志)Scale up rows,tuning type,then modelsize(前面都没问题的话)

Tip 9: Consider unsupervised fine-tuning if you've lots ofdata(如果数据量很大，可以考虑无监督微调)

Tip 10: Use preference fine-tuning(ORPO)(尝试使用偏好微调(ORPO))
