# 1. 笔记

### 1. 为什么要微调

1. foundation 大语言模型 在能够实现“普遍的”“一般性的”任务的“通用”，需要进行预训练。
2. 若想利用通用基座模型到自己下游业务场景中，若没有经过专属微调，直接拿来用的话，其实表现不如领域内已专门训练好的模型。 

### 2. 两种fine tune 范式
![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/f0505f42-095c-4be5-8493-64016bbdac3a)

1. **增量预训练**
   1. 基座模型（直接从hugging face下载的模型），让他学习一些<u>**新的知识**</u>
      1. 例子：我的垂直领域的一些常识
      2. 文章、书籍、代码等
   2. 不需要一问一答的学习，而是有监督、有标注的
   3. 例子：世界上第一高峰是珠穆朗玛
2. **指令跟随微调**
   1. 场景：让模型学会对话模板，根据人类的指令进行对话，让模型输出更符合人类喜好偏好的回答
   2. 训练数据：高质量的对话和问答数据
   3. 需要很高质量的标注（问答对）
   4. 例子：问：世界上第一高峰是什么？ 答：是珠穆朗玛峰啊

# 2. 实战

# 3. 微调的一些小技巧


Tip 1:Start withaSmall Model(从小模型开始，测试是否一切正常)

Tip 2: Use LoRA or OLoRA(使用 LoRA 或 OLoRA)

Tip 3: Create 10 manual questions(手动创建 10 个问题，测试是否一切正常)

Tip 4: Create datasets manually(手动创建数据集，会有更深的理解)

Tip 5: Start training withjust 100 rows(从只有 100 行开始训练，测试模型是否正常)

Tip 6: Always create a validation data split(始终创建验证数据集)

Tip 7: Start by only training on one GPU(开始时只使用一个GPU进行训练)

Tip 8: Use weights and biases for logging(记录日志)Scale up rows,tuning type,then modelsize(前面都没问题的话)

Tip 9: Consider unsupervised fine-tuning if you've lots ofdata(如果数据量很大，可以考虑无监督微调)

Tip 10: Use preference fine-tuning(ORPO)(尝试使用偏好微调(ORPO))
