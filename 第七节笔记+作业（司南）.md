# openCompass 司南评测

## 一. 笔记

### 1. 为什么要做评测？

1. 可以通过评测——测试——知道<u>LLM各自的优缺点</u>——促进LLM发展：所以需要不断地扩展评测维度、如数学、逻辑、代码、智能体等
2. 针对于垂直领域，需要有专业的知识才能应用到这些模型
   1. 比如医疗、金融、法律等
   2. 需要更专向的评估
3. 高质量中文基准
   1. 为了促进中文社区LLM的发展
   2. 针对中文场景进行评测
4. 有助于指导和改进人类与LLM之间的协同交互
5. 更好地规划大型语言模型未来的发展

### 2. 大模型评测的挑战

1. 评测能力还不足够全面
   1. 大模型场景太多了，也不断延伸出新的场景，每种场景都得评测。但目前还做不到每个场景都能评测好
   2. 大模型自己能力演进也太快了
   3. <u>对于可扩展的能力维度体系的建设</u>，是下一步要好好做的
2. 评测成本还是比较高
   1. 客观答题时，推理成本需要很多GPU资源
   2. 主观评测时，需要调用API接口，需要额外付费
   3. 人工打分成本也很高
3. 数据污染问题
   1. LLM如果在评测集上做训练，得分会比较高，但不符合实际了，不可信了。
   2. 所以<u>需要一种数据污染的检测技术</u>，在评测中要有体现。
   3. 如果受到了污染，应该收到提示
4. 鲁棒性
   1. 大模型对提示词很敏感
   2. 多次采样情况下模型性能不稳定

### 3. OC2.0司南大模型

1. 20240130，发布了2.0模型
2. 社区支持最完善的评测体系之一  
   1. 100+评测集  
   2. 50万+题目
   3. 支持 20+ HuggingFace 及 API 模型
   4. 覆盖学科、语言、知识、理解和推理
3. 提升了全能力维度的综合能力：考察大模型综合运用各类知识、理解和分析，并能多步推理
   1. 考试
   2. 对话
   3. 创作
   4. agent
   5. 评价
   6. 长文本
4. 自研数据集（不断更新、倾听hub需求）
   1. 专用模型反思能力：CriticBench 模型 
   2. 专用数学的：MathBench
   3. 专用模型使用工具能力的 ：T-Eval 
   4. 专用评测代码解释的：CIBench

### 4. 司南如何评测大模型

1. 根据大模型类型的不同去分别评测，有四种评测对象
   1. foundation模型
      1. 特点：未经过微调，经过海量的文本数据以自监督学习的方式进行训练获得的模型，具有强大的文字续写能力。（GPT3  LLaMA）
      2. 很多模型都是从基座模型训练过来的。需要设计一些方法去如何评测基座模型
   2. 对话模型
      1. 特点：在的基座模型的基础上，经过指令微调或人类偏好对齐获得的模型（ChatGPT、书生·浦语、通义千问）
   3. API模型
   4. 公开权重的开源模型
2. 评测题型设计（评测时，主观+客观结合评测）
   1. 客观评测：基于智能表达式，把模型的这个回答提取出来一个选项（建立固定答案），看有没有答对
      1. 问答题
      2. 选择题
   2. 主观评测——开放式
      1. 打分，直接打分模式的规则
      2. 直接说A比B好的这类规则
3. prompt engineer
   1. opencompass会自己把题目更丰富一些——给模型做推理——做测评——结果更真实的反映模型新功能![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/bc68b63e-502e-4045-8163-078397f93dc1)
4. 小样本学习
5. 思维链技术
6. 长文本评测（能够让模型大海捞针）
   1. input：在很长很长的这样一个文档里面，在某一个位置突然插入一个就是不相关的一句话，比如说小明在上海人工智能实验室学习机给它插到红楼梦里面，然后你让模型读完了这本书之后，然后你再问小明在哪里实习？
   2. 分析：指令跟随能力  |  长文本建模能力  | 信息抽取能力
   3. output：上海人工智能实验室
   4. 结果：如果模型如果能回答出来，那就证明他的这个长文本能力很好
7. 汇集社区力量
   1. 工具——基准——榜单  三位一体
   2. compassHub : 提供高时效性高质量评测集
   3. compassRank：发布权威榜单 、洞悉行业趋势
   4. compassKit ：全栈测评工具，支持高效评测和能力分析
   5. 以上三个互相循环：hub -> rank -> kit -> hub
   6. 已经有100+大模型加入了司南的评测
   7. 还有多模态的榜单

### 5. compassKit：全栈工具链

1. 基本功能：
   1. 数据污染检查
   2. 长文本能力
   3. 更丰富的模型推理接入
      1. 评测的时候用 hugging face 的这个模型，它可能评测的速度比较慢
      2. 直接支持去更换推理模型后端：比如LMdeploy
      3. 更换了这些后完之后你的模型推理速度就会变得非常快
   4. 中英文双语主观评测![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/caa08d98-370a-4731-8474-469d9b35d4e5)

2. 评测流水线：

   1. 特点：
      1. 自定义模型、数据集
      2. 能切分模型、数据集
      3. 能做并行化：充分利用多GPU能力
      4. 结果支持多种输出方案，TXT也能输出
   2. 配置——推理——评估——可视化
   3. 在 OpenCompass 中评估一个模型通常包括以下几个阶段：配置 -> 推理 -> 评估 -> 可视化。
      - 配置：
        - 工作流起点。
        - must：选择要评估的模型和数据集 
        - optional：①选择评估策略、计算后端等     ②定义显示结果的方式
      - 推理与评估：
        - ① 对模型和数据集进行并行推理和评估
        - ② 推理阶段：让模型从数据集产生输出
        - ③ 评估阶段：衡量输出与标准答案的匹配程度
        - 特点：这两个过程会被拆分为多个同时运行的“任务”以提高效率
        - 注意：如果计算资源有限，这种策略可能会使评测变得更慢
      - 可视化：
      - ① 评估完成后，OpenCompass 将结果整理成易读的表格，并将其保存为 CSV 和 TXT 文件。
      - ② 也可以利用飞书：在飞书客户端中及时获得评测状态报告

   ![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/b1cf2d69-f296-466c-b0cb-c0ec24c1dd82)

### 6. compassKit其他特点

1. 多模态评测工具 VLMEvalKit：一站式多模态评测工具，支持主流多模态模型和数据集。
2. 代码评测工具 Code-Evaluator: 提供基于docker的统一编程语言评测环境，确保代码评测的稳定性和可复现性。
3. MoE模型入门工具 MixtralKit: 为MoE模型初学者提供学习资料、模型架构解析、推理与评测教程

## 二. 实战

展示书生浦语在 `C-Eval` 基准任务上的评估。它们的配置文件可以在 `configs/eval_demo.py` 中找到。