# openCompass 司南评测

## 一. 笔记

### 1. 为什么要做评测？

1. 可以通过评测——测试——知道LLM各自的优缺点——促进LLM发展：所以需要不断地扩展评测维度、如数学、逻辑、代码、智能体等
2. 针对于垂直领域，需要有专业的知识才能应用到这些模型
   1. 比如医疗、金融、法律等
   2. 需要更专向的评估
3. 高质量中文基准
   1. 为了促进中文社区LLM的发展
   2. 针对中文场景进行评测
4. 反哺能力迭代
   1. 其实和第一点差不多，就是通过评测发现不足，发展不足

### 2. 大模型评测的挑战

1. 评测能力还不足够全面
   1. 大模型场景太多了，也不断延伸出新的场景，每种场景都得评测。但目前还做不到每个场景都能评测好
   2. 大模型自己能力演进也太快了
   3. <u>对于可扩展的能力维度体系的建设</u>，是下一步要好好做的
2. 评测成本还是比较高
   1. 客观答题时，推理成本需要很多GPU资源
   2. 主观评测时，需要调用API接口，需要额外付费
   3. 人工打分成本也很高
3. 数据污染问题
   1. LLM如果在评测集上做训练，得分会比较高，但不符合实际了，不可信了。
   2. 所以<u>需要一种数据污染的检测技术</u>，在评测中要有体现。
   3. 如果受到了污染，应该收到提示
4. 鲁棒性
   1. 大模型对提示词很敏感
   2. 多次采样情况下模型性能不稳定

### 3. OC2.0司南大模型

1. 20240130，发布了2.0模型
2. 社区支持最完善的评测体系之一  
   1. 100+评测集  
   2. 50万+题目
   3. 覆盖学科、语言、知识、理解和推理

### 4. 司南如何评测大模型

1. 根据大模型类型的不同去分别评测
   1. foundation模型
      1. 未经过微调
      2. 很多模型都是从基座模型训练过来的
      3. 需要设计一些方法去如何评测基座模型
   2. 对话模型
      1. 类似于通义千问等开源模型
   3. API模型
   4. 公开权重的开源模型
2. 评测题型设计
   1. 客观评测：基于智能表达式，把模型的这个回答提取出来一个选项（建立固定答案），看有没有答对
      1. 问答题
      2. 选择题
   2. 主观评测——开放式
      1. 打分，直接打分模式的规则
      2. 直接说A比B好的这类规则
3. prompt engineer
   1. opencompass会自己把题目更丰富一些——给模型做推理——做测评——结果更真实的反映模型新功能

## 二. 实战