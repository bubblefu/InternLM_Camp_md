# openCompass 司南评测

## 一. 笔记

### 1. 为什么要做评测？

1. 可以通过评测——测试——知道<u>LLM各自的优缺点</u>——促进LLM发展：所以需要不断地扩展评测维度、如数学、逻辑、代码、智能体等
2. 针对于垂直领域，需要有专业的知识才能应用到这些模型
   1. 比如医疗、金融、法律等
   2. 需要更专向的评估
3. 高质量中文基准
   1. 为了促进中文社区LLM的发展
   2. 针对中文场景进行评测
4. 有助于指导和改进人类与LLM之间的协同交互
5. 更好地规划大型语言模型未来的发展

### 2. 大模型评测的挑战

1. 评测能力还不足够全面
   1. 大模型场景太多了，也不断延伸出新的场景，每种场景都得评测。但目前还做不到每个场景都能评测好
   2. 大模型自己能力演进也太快了
   3. <u>对于可扩展的能力维度体系的建设</u>，是下一步要好好做的
2. 评测成本还是比较高
   1. 客观答题时，推理成本需要很多GPU资源
   2. 主观评测时，需要调用API接口，需要额外付费
   3. 人工打分成本也很高
3. 数据污染问题
   1. LLM如果在评测集上做训练，得分会比较高，但不符合实际了，不可信了。
   2. 所以<u>需要一种数据污染的检测技术</u>，在评测中要有体现。
   3. 如果受到了污染，应该收到提示
4. 鲁棒性
   1. 大模型对提示词很敏感
   2. 多次采样情况下模型性能不稳定

### 3. OC2.0司南大模型

1. 20240130，发布了2.0模型
2. 社区支持最完善的评测体系之一  
   1. 100+评测集  
   2. 50万+题目
   3. 支持 20+ HuggingFace 及 API 模型
   4. 覆盖学科、语言、知识、理解和推理
3. 提升了全能力维度的综合能力：考察大模型综合运用各类知识、理解和分析，并能多步推理
   1. 考试
   2. 对话
   3. 创作
   4. agent
   5. 评价
   6. 长文本
4. 自研数据集（不断更新、倾听hub需求）
   1. 专用模型反思能力：CriticBench 模型 
   2. 专用数学的：MathBench
   3. 专用模型使用工具能力的 ：T-Eval 
   4. 专用评测代码解释的：CIBench

### 4. 司南如何评测大模型

1. 根据大模型类型的不同去分别评测，有四种评测对象
   1. foundation模型
      1. 特点：未经过微调，经过海量的文本数据以自监督学习的方式进行训练获得的模型，具有强大的文字续写能力。（GPT3  LLaMA）
      2. 很多模型都是从基座模型训练过来的。需要设计一些方法去如何评测基座模型
   2. 对话模型
      1. 特点：在的基座模型的基础上，经过指令微调或人类偏好对齐获得的模型（ChatGPT、书生·浦语、通义千问）
   3. API模型
   4. 公开权重的开源模型
2. 评测题型设计（评测时，主观+客观结合评测）
   1. 客观评测：基于智能表达式，把模型的这个回答提取出来一个选项（建立固定答案），看有没有答对
      1. 问答题
      2. 选择题
   2. 主观评测——开放式
      1. 打分，直接打分模式的规则
      2. 直接说A比B好的这类规则
3. prompt engineer
   1. opencompass会自己把题目更丰富一些——给模型做推理——做测评——结果更真实的反映模型新功能![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/bc68b63e-502e-4045-8163-078397f93dc1)
4. 小样本学习
5. 思维链技术
6. 长文本评测（能够让模型大海捞针）
   1. input：在很长很长的这样一个文档里面，在某一个位置突然插入一个就是不相关的一句话，比如说小明在上海人工智能实验室学习机给它插到红楼梦里面，然后你让模型读完了这本书之后，然后你再问小明在哪里实习？
   2. 分析：指令跟随能力  |  长文本建模能力  | 信息抽取能力
   3. output：上海人工智能实验室
   4. 结果：如果模型如果能回答出来，那就证明他的这个长文本能力很好
7. 汇集社区力量
   1. 工具——基准——榜单  三位一体
   2. compassHub : 提供高时效性高质量评测集
   3. compassRank：发布权威榜单 、洞悉行业趋势
   4. compassKit ：全栈测评工具，支持高效评测和能力分析
   5. 以上三个互相循环：hub -> rank -> kit -> hub
   6. 已经有100+大模型加入了司南的评测
   7. 还有多模态的榜单

### 5. compassKit：全栈工具链

1. 基本功能：
   1. 数据污染检查
   2. 长文本能力
   3. 更丰富的模型推理接入
      1. 评测的时候用 hugging face 的这个模型，它可能评测的速度比较慢
      2. 直接支持去更换推理模型后端：比如LMdeploy
      3. 更换了这些后完之后你的模型推理速度就会变得非常快
   4. 中英文双语主观评测![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/caa08d98-370a-4731-8474-469d9b35d4e5)

2. 评测流水线：

   1. 特点：
      1. 自定义模型、数据集
      2. 能切分模型、数据集
      3. 能做并行化：充分利用多GPU能力
      4. 结果支持多种输出方案，TXT也能输出
   2. 配置——推理——评估——可视化
   3. 在 OpenCompass 中评估一个模型通常包括以下几个阶段：配置 -> 推理 -> 评估 -> 可视化。
      - 配置：
        - 工作流起点。
        - must：选择要评估的模型和数据集 
        - optional：①选择评估策略、计算后端等     ②定义显示结果的方式
      - 推理与评估：
        - ① 对模型和数据集进行并行推理和评估
        - ② 推理阶段：让模型从数据集产生输出
        - ③ 评估阶段：衡量输出与标准答案的匹配程度
        - 特点：这两个过程会被拆分为多个同时运行的“任务”以提高效率
        - 注意：如果计算资源有限，这种策略可能会使评测变得更慢
      - 可视化：
      - ① 评估完成后，OpenCompass 将结果整理成易读的表格，并将其保存为 CSV 和 TXT 文件。
      - ② 也可以利用飞书：在飞书客户端中及时获得评测状态报告

   ![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/b1cf2d69-f296-466c-b0cb-c0ec24c1dd82)

### 6. compassKit其他特点

1. 多模态评测工具 VLMEvalKit：一站式多模态评测工具，支持主流多模态模型和数据集。
2. 代码评测工具 Code-Evaluator: 提供基于docker的统一编程语言评测环境，确保代码评测的稳定性和可复现性。
3. MoE模型入门工具 MixtralKit: 为MoE模型初学者提供学习资料、模型架构解析、推理与评测教程

## 二. 实战

展示书生浦语在 `C-Eval` 基准任务上的评估。它们的配置文件可以在 `configs/eval_demo.py` 中找到。



```
python              \ 
run.py              \           
--datasets ceval_gen \
--hf-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b \  # 指定要评测的HuggingFace 模型路径
--tokenizer-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b \  # 制定要评测的HuggingFace模型 的 tokenizer 路径（如果与模型路径相同，可以省略）
--tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True \  # 构建 tokenizer 的参数
--model-kwargs device_map='auto' trust_remote_code=True \  # 构建模型的参数
--max-seq-len 2048 \  # 模型可以接受的最大token序列长度，长上下文要更大一些， 一般2048足够
--max-out-len 16 \  # 生成的最大 token 数，客观评测多，模型就答ABCD那种，也不需要开很大，可以加快模型推理速度；主观评测需要比较长（512这种）
--batch-size 2  \  # 批量大小
--num-gpus 1  \ # 运行模型所需的 GPU 数量
--reuse latest  \  # 指定最新或者选定时间戳，会复用已经存在的结果，继续去跑；比如数据集跑了一半，后边断掉了，那就可以继续去跑
--work-dir 'xxxx/xxxx' \  # 在这个路径下执行相关操作，工作路径，所有结果日志都会保存在这个路径下；default的话，就会在output/default下
--debug  # 进入debug模式，所有信息都在terminal里显示  如果不写这个，debug信息会在default/log下
```



python run.py --datasets ceval_gen --hf-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b --tokenizer-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b --tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True --model-kwargs trust_remote_code=True device_map='auto' --max-seq-len 1024 --max-out-len 16 --batch-size 2 --num-gpus 1 --debug    这些参数分别是什么意思 



ChatGPT

这些参数在运行Python脚本`run.py`时分别配置了不同的选项，下面是每个参数的详细解释：

1. **`--datasets ceval_gen`**:
   - 指定要使用的数据集名称。这里使用的是`ceval_gen`数据集。
2. **`--hf-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b`**:
   - 指定Hugging Face模型路径。在这个路径下会找到名为`internlm2-chat-1_8b`的模型。
3. **`--tokenizer-path /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b`**:
   - 指定分词器（tokenizer）的路径。这里与模型路径相同。
4. **`--tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True`**:
   - `padding_side='left'`：设置分词器在进行padding时从左边进行。
   - `truncation='left'`：设置分词器在进行截断时从左边进行。
   - `trust_remote_code=True`：信任远程代码，这通常用来加载需要运行远程代码的自定义分词器或模型。
5. **`--model-kwargs trust_remote_code=True device_map='auto'`**:
   - `trust_remote_code=True`：信任远程代码，类似于分词器的选项。
   - `device_map='auto'`：自动分配设备（如GPU或CPU）来运行模型。
6. **`--max-seq-len 1024`**:
   - 指定输入序列的最大长度为1024个标记（tokens）。
7. **`--max-out-len 16`**:
   - 指定模型生成输出的最大长度为16个标记（tokens）。
8. **`--batch-size 2`**:
   - 指定批处理大小为2。也就是说，每次处理两个输入样本。
9. **`--num-gpus 1`**:
   - 指定使用的GPU数量为1。
10. **`--debug`**:
    - 开启调试模式。通常会提供更多的日志信息，有助于排查问题。

总结一下，这些参数配置了数据集、模型路径、分词器路径及其参数、模型参数、输入和输出序列的最大长度、批处理大小、GPU数量，以及是否开启调试模式等。

![image](https://github.com/bubblefu/InternLM_Camp_md/assets/70378994/3cfe803e-483b-4568-baee-44f26c3dd4ee)
